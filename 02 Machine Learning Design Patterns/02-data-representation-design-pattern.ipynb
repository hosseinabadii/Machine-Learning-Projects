{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Representation Design Patterns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of any machine learning model is a mathematical function that\n",
    "is defined to operate on specific types of data only. At the same time, real-\n",
    "world machine learning models need to operate on data that may not be\n",
    "directly pluggable into the mathematical function.\n",
    "\n",
    "The process of creating features to represent the input data is called `feature engineering`, and so we can think of feature engineering as a way of selecting the data representation.\n",
    "\n",
    "The process of learning features to represent the input data is called `feature extraction`, and we can think of learnable data representations (like embeddings) as automatically engineered features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data representation design patterns include:\n",
    "\n",
    "1. `Hashed Feature`  \n",
    "It involves encoding categorical inputs as unique strings and hashing them.\n",
    "\n",
    "2. `Embeddings`  \n",
    "It's a technique for representing high-cardinality data such as inputs with many possible categories or text data. Embeddings represent data in multidimensional space, where the dimension is dependent on our data and prediction task.\n",
    "\n",
    "3. `Feature Cross`  \n",
    "It's an approach that joins two features to extract relationships that may not have been easily captured by encoding the features on their own.\n",
    "\n",
    "4. `Multimodal Input`  \n",
    "It addresses the problem of how to combine inputs of different types into the same model, and how a single feature can be represented multiple ways."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: Hashed Feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hashed Feature design pattern addresses three possible problems\n",
    "associated with categorical features:\n",
    "- incomplete vocabulary\n",
    "- model size due to cardinality\n",
    "- cold start.\n",
    "\n",
    "It does so by grouping the categorical\n",
    "features and accepting the trade-off of collisions in the data representation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "One-hot encoding a categorical input variable requires knowing the\n",
    "vocabulary beforehand. This is not a problem if the input variable is\n",
    "something like the language a book is written in or the day of the week\n",
    "that traffic level is being predicted.\n",
    "\n",
    "What if the categorical variable in question is something like the\n",
    "hospital_id of where the baby is born or the physician_id of the\n",
    "person delivering the baby? Categorical variables like these pose a few problems:\n",
    "\n",
    "- Knowing the vocabulary requires extracting it from the training\n",
    "data. Due to random sampling, it is possible that the training data\n",
    "does not contain all the possible hospitals or physicians. The\n",
    "vocabulary might be incomplete.\n",
    "\n",
    "- The categorical variables have high cardinality. Instead of havingfeature vectors with three languages or seven days, we have\n",
    "feature vectors whose length is in the thousands to millions. Such\n",
    "feature vectors pose several problems in practice. They involve so\n",
    "many weights that the training data may be insufficient. Even if\n",
    "we can train the model, the trained model will require a lot of\n",
    "space to store because the entire vocabulary is needed at serving\n",
    "time. Thus, we may not be able to deploy the model on smaller\n",
    "devices.\n",
    "\n",
    "- After the model is placed into production, new hospitals might be\n",
    "built and new physicians hired. The model will be unable to make\n",
    "predictions for these, and so a separate serving infrastructure will\n",
    "be required to handle such cold-start problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "The Hashed Feature design pattern represents a categorical input variable\n",
    "by doing the following:\n",
    "\n",
    "1. Converting the categorical input into a unique string.\n",
    "\n",
    "2. Invoking a deterministic (no random seeds or salt) and portable\n",
    "(so that the same algorithm can be used in both training and\n",
    "serving) hashing algorithm on the string.\n",
    "\n",
    "3. Taking the remainder when the hash result is divided by the desired number of buckets. Typically the hashing algorithm returns an integer that can be negative and the modulo of a negative integer is negative. So, the absolute value of the result is taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Hashed Feature design pattern, we have to use a fingerprint hashing\n",
    "algorithm and not a cryptographic hashing algorithm. This is because the\n",
    "goal of a fingerprint function is to produce a deterministic and unique\n",
    "value. If you think about it, this is a key requirement of preprocessing\n",
    "functions in machine learning, since we need to apply the same function\n",
    "during model serving and get the same hashed value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2: Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are a learnable data representation that map high-cardinality data into a lower-dimensional space in such a way that the information relevant to the learning problem is preserved. Embeddings are at the heart of modern-day machine learning and have various incarnations throughout\n",
    "the field."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models systematically look for patterns in data that capture how the properties of the model’s input features relate to the output label. As a result, the data representation of the input features directly affects the quality of the final model.\n",
    "\n",
    "While handling structured, numeric input is fairly straightforward, the data needed to train a machine learning model can come in myriad varieties, such as categorical features, text, images, audio, time series, and many more.\n",
    "\n",
    "For these data representations, we need a meaningful numeric value to supply our machine learning model so these features can fit within the typical training paradigm.\n",
    "\n",
    "Embeddings provide a way to handle some of these disparate data types in a way that preserves similarity between items and thus improves our model’s ability to learn those essential patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "The Embeddings design pattern addresses the problem of representing high-cardinality data densely in a lower dimension by passing the input data through an embedding layer that has trainable weights.\n",
    "\n",
    "This will map the high-dimensional, categorical input variable to a real-valued vector in some low-dimensional space. The weights to create the dense representation are learned as part of the optimization of the model. In practice, these embeddings end up capturing closeness\n",
    "relationships in the input data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3: Feature Cross"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Feature Cross design pattern helps models learn relationships between\n",
    "inputs faster by explicitly making each combination of input values a\n",
    "separate feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "Consider the dataset in Figure below and the task of creating a binary classifier that separates the + and − labels.\n",
    "\n",
    "<img src=\"images/feature-cross-problem.png\" width=400>\n",
    "\n",
    "Using only the x_1 and x_2 coordinates, it is not possible to find a linear boundary that separates the + and − classes. This means that to solve this problem, we have to make the model more\n",
    "complex, perhaps by adding more layers to the model. However, a simpler\n",
    "solution exists."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, feature engineering is the process of using domain knowledge to create new features that aid the machine learning process and increase the predictive power of our model. One commonly used feature engineering technique is creating a feature cross."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A feature cross is a synthetic feature formed by concatenating two or more categorical features in order to capture the interaction between them. By joining two features in this way, it is possible to encode nonlinearity into the model, which can allow for predictive abilities beyond what each of the features would have been able to provide individually."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature crosses provide a way to have the ML model learn relationships between the features faster. While more complex models like neural networks and trees can learn feature crosses on their own, using feature crosses explicitly can allow us to get away with training just a linear model.\n",
    "\n",
    "Consequently, feature crosses can speed up model training (less expensive) and reduce model complexity (less training data is needed)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4: Multimodal Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multimodal Input design pattern addresses the problem of representing different types of data or data that can be expressed in complex ways by concatenating all the available data representations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Typically, an input to a model can be represented as a number or as a category, an image, or free-form text. Many off-the-shelf models are defined for specific types of input only—a standard image classification model such as Resnet-50, for example, does not have the ability to handle inputs other than images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the need for multimodal inputs, let’s say we’ve got a camera capturing footage at an intersection to identify traffic violations. We want our model to handle both image data (camera footage) and some metadata about when the image was captured (time of day, day of week, weather, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem also occurs when training a structured data model where one of the inputs is free-form text. Unlike numerical data, images and text cannot be fed directly into a model. As a result, we’ll need to represent image and text inputs in a way our model can understand (usually using the  Embeddings design pattern), then combine these inputs with other tabular features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we might want to predict a restaurant patron’s rating based on their review text and other attributes such as what they paid and whether it was lunch or dinner.\n",
    "\n",
    "<img src=\"images/multimodal-input.png\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll first combine the numerical and categorical features. There are three possible options for meal_type, so we can turn this into a one-hot encoding and will represent dinner as `[0, 0, 1]`. With this categorical feature represented as an array, we can now combine it with meal_total by adding the price of the meal as the fourth element of the array: `[0, 0, 1, 30.5]`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embeddings design pattern is a common approach to encoding text for machine learning models. Then, we need to flatten the embedding in order to concatenate with the meal_type and meal_total.\n",
    "\n",
    "We could then use a series of Dense layers to transform that very large array into smaller ones ending with our output that is an array of three numbers.\n",
    "\n",
    "We now need to concatenate these three numbers, which form the sentence embedding of the review with the earlier inputs:\n",
    "```\n",
    "[0, 0, 1, 30.5, 0.75, -0.82, 0.45]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
